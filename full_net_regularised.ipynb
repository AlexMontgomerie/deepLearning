{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMontgomerie/deepLearning/blob/master/full_net_regularised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tSMVa8-rncVS",
        "colab_type": "code",
        "outputId": "4cca606f-bf32-4d4c-f7f1-bf18008fd8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!rm -rf deepLearning\n",
        "!git clone https://github.com/AlexMontgomerie/deepLearning\n",
        "%cd deepLearning\n",
        "!git pull origin master\n",
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'deepLearning' already exists and is not an empty directory.\n",
            "/content/deepLearning\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/AlexMontgomerie/deepLearning\n",
            " * branch            master     -> FETCH_HEAD\n",
            "   2744968..05a47de  master     -> origin/master\n",
            "Updating 2744968..05a47de\n",
            "Fast-forward\n",
            " read_data.py | 4 \u001b[32m+++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 3 insertions(+), 1 deletion(-)\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "('RAM Free: 11.4 GB', ' | Proc size: 4.9 GB')\n",
            "GPU RAM Free: 11193MB | Used: 248MB | Util   2% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWqnNDkSnqfz",
        "colab_type": "code",
        "outputId": "65ab231c-8951-43c6-fea7-b91cc57dfc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "from common import *\n",
        "!./setup.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--2019-03-12 06:35:55--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 103.116.4.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-12 06:35:56--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-12 06:35:56--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 107.152.24.199, 107.152.25.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|107.152.24.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!J_l25PTaeG4Wz4kMjY8qmw5d3nkLMMOo2nBZDNctU7fkgd7H2gsRbuKLcpy4dzagunMe0WmdafPGsxZBFyuN4Wx-fvyPgl9lSgj7oC48urDNXo9VpWN9KxFyMo5loZ1HzR6kPw-2BvsOJaEwXeT3XPbkes_jnQJmtAKjwaWpUo4bmqKggaVPBGSdzP50HdnIdxFkBTnXPyLlpyuJySv5pgaoaj8M8DZLUb_k3URkPJB--2ai60RKl07oesbSK7xnZzMrrj3QsPNdUGwL9lmOsLvsUClld0TT_TXdRPOZeQW9fhRZoUB_6NgIwNi1ZCVSpvbVjCNyT9DjtKyCx6P26dQFqFjnLGFxwSRwAecbPdT-SxmG8dB0Bc8T6Yo-sinqMc_LpaZdZurxCkkGzFCiQpDtpdfeHFPVBlS0gaZNcuq0EBdOa__BCibKQaQ9GQn5PDWgwduNgXXqMqE_Hqjg0NQIKRpe7GIc1X4QvUibIwKxytUQ7WrLku359aGYuBjoHRxOmSWnIdlJo9Vc4-S0lZoG5mWnlzEFqpnrjCSDjFuaeKMtKgSEZYsYMLOyRjTCSKCpwdN77QzSX4ZwhqpapgThPPYAHvCB8WwogStcm5SI3z_tVje_uL-5B_dIqyulc9mftpf7JAE0wWUrzZtwHZVYnnRgZb5SYUGh8TjgTbGuP5e1FliwNzw3_u4dp6sTaZA5byQpE_-w2A7A6vijM1U0te2Rz6t70YTyDwQjwo4KRs5-O_oZ257Obm3OiNEWu3zGluTcNGaax_RM1VSMz8r_ylHopLUEchNk4l-ARPzc_L3Qu-0fwMuhrh0C1K_nW1zLyaToknRDA_b63Is5xvANQlqofYhrvkHvsWkMnWbfcMdI13_1uyBezxLFI14kpj7LLUS3UuhrTmXx5wrqodv_GjsK0TElUcC2BmznCcPXrAPUuC_WdtkLSDHc0DQ-6ZBeaw5EcVVlLeyh9fckljoPkvXbALw8CTnnr7T3ZdX95DXw67uUHuf3OPTyZNsIDBvaTJ7lODsAB4zhbSsNqhznllrlxSekSe6kuCnDrVgrvn3NtkYPKMI1vo3SMR2ITZy5ueME_NlejrIQEXEnunksHzByPc-RuDxLzaDQwVaS1N-f0TiJZPB02DZGP5J3kjJxcJW2sM5NSrivIHELfUkGYMCbuI0EcUhERaL0KEiePneTqE2bS_N8ivfShCZkEYx5sx4zqYBElnghYisHOrgcwoOBiSDhLV3tJg8reJm7ZYfUYcD_2ZhagIrqau3Qj68bxsWBYA-fZJHsSQ2z-OHxCsljy8bONbpvYclqC5yOGS_mTIFI-aMbPXPj_tmNV3g2uSY4gB0s8IWGR7ClvQOjteDlfLCADeQXXGx5MdNGt4gin4xAxHzjuWTJQY1km9eA8oO4EaApE-LUwonDSAKbTfvbcvnWNOyBbXlZiMZwMLb_MfB3TyNyL3nsqTRiS1Gn-94pALes8pkx9yC3lan0aQzeX7XcQMHEuw../download [following]\n",
            "--2019-03-12 06:35:57--  https://public.boxcloud.com/d/1/b1!J_l25PTaeG4Wz4kMjY8qmw5d3nkLMMOo2nBZDNctU7fkgd7H2gsRbuKLcpy4dzagunMe0WmdafPGsxZBFyuN4Wx-fvyPgl9lSgj7oC48urDNXo9VpWN9KxFyMo5loZ1HzR6kPw-2BvsOJaEwXeT3XPbkes_jnQJmtAKjwaWpUo4bmqKggaVPBGSdzP50HdnIdxFkBTnXPyLlpyuJySv5pgaoaj8M8DZLUb_k3URkPJB--2ai60RKl07oesbSK7xnZzMrrj3QsPNdUGwL9lmOsLvsUClld0TT_TXdRPOZeQW9fhRZoUB_6NgIwNi1ZCVSpvbVjCNyT9DjtKyCx6P26dQFqFjnLGFxwSRwAecbPdT-SxmG8dB0Bc8T6Yo-sinqMc_LpaZdZurxCkkGzFCiQpDtpdfeHFPVBlS0gaZNcuq0EBdOa__BCibKQaQ9GQn5PDWgwduNgXXqMqE_Hqjg0NQIKRpe7GIc1X4QvUibIwKxytUQ7WrLku359aGYuBjoHRxOmSWnIdlJo9Vc4-S0lZoG5mWnlzEFqpnrjCSDjFuaeKMtKgSEZYsYMLOyRjTCSKCpwdN77QzSX4ZwhqpapgThPPYAHvCB8WwogStcm5SI3z_tVje_uL-5B_dIqyulc9mftpf7JAE0wWUrzZtwHZVYnnRgZb5SYUGh8TjgTbGuP5e1FliwNzw3_u4dp6sTaZA5byQpE_-w2A7A6vijM1U0te2Rz6t70YTyDwQjwo4KRs5-O_oZ257Obm3OiNEWu3zGluTcNGaax_RM1VSMz8r_ylHopLUEchNk4l-ARPzc_L3Qu-0fwMuhrh0C1K_nW1zLyaToknRDA_b63Is5xvANQlqofYhrvkHvsWkMnWbfcMdI13_1uyBezxLFI14kpj7LLUS3UuhrTmXx5wrqodv_GjsK0TElUcC2BmznCcPXrAPUuC_WdtkLSDHc0DQ-6ZBeaw5EcVVlLeyh9fckljoPkvXbALw8CTnnr7T3ZdX95DXw67uUHuf3OPTyZNsIDBvaTJ7lODsAB4zhbSsNqhznllrlxSekSe6kuCnDrVgrvn3NtkYPKMI1vo3SMR2ITZy5ueME_NlejrIQEXEnunksHzByPc-RuDxLzaDQwVaS1N-f0TiJZPB02DZGP5J3kjJxcJW2sM5NSrivIHELfUkGYMCbuI0EcUhERaL0KEiePneTqE2bS_N8ivfShCZkEYx5sx4zqYBElnghYisHOrgcwoOBiSDhLV3tJg8reJm7ZYfUYcD_2ZhagIrqau3Qj68bxsWBYA-fZJHsSQ2z-OHxCsljy8bONbpvYclqC5yOGS_mTIFI-aMbPXPj_tmNV3g2uSY4gB0s8IWGR7ClvQOjteDlfLCADeQXXGx5MdNGt4gin4xAxHzjuWTJQY1km9eA8oO4EaApE-LUwonDSAKbTfvbcvnWNOyBbXlZiMZwMLb_MfB3TyNyL3nsqTRiS1Gn-94pALes8pkx9yC3lan0aQzeX7XcQMHEuw../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.25.200, 107.152.24.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.25.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip   100%[===================>]   3.81G  21.8MB/s    in 3m 3s   \n",
            "\n",
            "2019-03-12 06:39:01 (21.3 MB/s) - ‘hpatches_data.zip’ saved [4088106554/4088106554]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0YF2io-cnthT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Input, UpSampling2D, concatenate, Subtract\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "#from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, STNHPatches, tps\n",
        "from read_data import *\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "import matplotlib.pyplot as plt\n",
        "from layers import BilinearInterpolation\n",
        "from keras.layers import Layer, Lambda\n",
        "\n",
        "def run_sobel(image):\n",
        "  return tf.image.sobel_edges(image)[:,:,:,0]\n",
        "      \n",
        "\n",
        "def get_initial_weights(output_size):\n",
        "    b = np.zeros((2, 3), dtype='float32')\n",
        "    b[0, 0] = 1\n",
        "    b[1, 1] = 1\n",
        "    W = np.zeros((output_size, 6), dtype='float32')\n",
        "    weights = [W, b.flatten()]\n",
        "    return weights\n",
        "  \n",
        "def get_full_model(shape,stn_init=None):  \n",
        "    #shape = (32, 32, 1)\n",
        "\n",
        "    init_weights = keras.initializers.he_normal()\n",
        "    # input \n",
        "    inputs = Input(shape)\n",
        "    \n",
        "    # denoise network\n",
        "    depth1  = 32\n",
        "    conv1_1 = Conv2D(depth1, 1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "\n",
        "    # convolution layers\n",
        "    conv1_2  = Conv2D(depth1, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_3  = Conv2D(depth1, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_5  = Conv2D(depth1, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_7  = Conv2D(depth1, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_9  = Conv2D(depth1, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_11 = Conv2D(depth1, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "\n",
        "    # network\n",
        "    net1 = Subtract()([conv1_1, conv1_2])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_3])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_5])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_7])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_9])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_11])\n",
        "    net1 = BatchNormalization()(net1)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth2 = 16\n",
        "    conv2_1  = Conv2D(depth2, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv2_2  = Conv2D(depth2, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_3  = Conv2D(depth2, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_5  = Conv2D(depth2, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_7  = Conv2D(depth2, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_9  = Conv2D(depth2, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_11 = Conv2D(depth2, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "\n",
        "    # network\n",
        "    net2 = Subtract()([conv2_1, conv2_2])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_3])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_5])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_7])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_9])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_11])\n",
        "    net2 = BatchNormalization()(net2)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth3 = 8\n",
        "    conv3_1  = Conv2D(depth3, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv3_2  = Conv2D(depth3, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_3  = Conv2D(depth3, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_5  = Conv2D(depth3, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_7  = Conv2D(depth3, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_9  = Conv2D(depth3, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_11 = Conv2D(depth3, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "\n",
        "    # network\n",
        "    net3 = Subtract()([conv3_1, conv3_2])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_3])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_5])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_7])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_9])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_11])\n",
        "    net3 = BatchNormalization()(net3)  \n",
        "\n",
        "    #net = Conv2D(1, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(net3)\n",
        "  \n",
        "    \n",
        "    # stn network    \n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(net3)\n",
        "    locnet = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = Flatten()(locnet)\n",
        "    locnet = Dense(100)(locnet)\n",
        "    locnet = Activation('sigmoid')(locnet)\n",
        "    weights = get_initial_weights(100)\n",
        "    locnet = Dense(6, weights=weights)(locnet)\n",
        "    stn    = BilinearInterpolation(shape[:-1])([net3, locnet])\n",
        "    \n",
        "    if stn_init:\n",
        "      stn.set_weights(stn_init.get_weights())\n",
        "      \n",
        "    # sobel\n",
        "    sobel = Lambda(run_sobel)(net3)\n",
        "    \n",
        "    # features in\n",
        "    l2net = concatenate([ net3, stn , sobel ], axis = -1)\n",
        "    \n",
        "    # L2 Net    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 8, padding='valid', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "\n",
        "    l2net = Reshape((128,))(l2net)\n",
        "  \n",
        "    l2net = Model(inputs = inputs, outputs = l2net)\n",
        "    \n",
        "    descriptor_model = Sequential()\n",
        "    descriptor_model.add(l2net)\n",
        "    \n",
        "    return descriptor_model\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4HJeu6vBnQZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "\n",
        "def triplet_loss(x):\n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  \n",
        "  # Suppose `SomeKernel` acts on vectors (rank-1 tensors)\n",
        "  #scalar_kernel = tfp.positive_semidefinite_kernels.ExponentiatedQuadratic(feature_ndims=output_dim)\n",
        "  #positive_distance = scalar_kernel.apply(a,p)\n",
        "  #negative_distance = scalar_kernel.apply(a,n)\n",
        "  \n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "sgd = keras.optimizers.SGD(lr=0.1)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N6a8ibZjnR8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "alphaIn = Input(shape=(1,), name='alpha')\n",
        "\n",
        "def triplet_loss_regularised(x):  \n",
        "  a, p, n, _alpha = x\n",
        "\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "\n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha[0]), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss_regularised)([ea, ep, en, alphaIn])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn, alphaIn], outputs=loss)\n",
        "sgd = keras.optimizers.SGD(lr=0.1)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgVmVZrHobrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000, batch_size=500)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000, batch_size=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjpyrKsnnbje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bc0823b6-b1e7-4054-fca2-afc4285fd211"
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]\n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs))\n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatchesRegularised(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDescRegularised(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000, batch_size=500)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDescRegularised(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000, batch_size=500)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using noisy patches\n",
            "100%|██████████| 116/116 [00:32<00:00,  3.09it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 1323/100000 [00:00<00:07, 13226.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:01<00:00, 56237.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using noisy patches\n",
            "100%|██████████| 116/116 [00:19<00:00,  5.94it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███       | 3068/10000 [00:00<00:00, 30678.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10000/10000 [00:00<00:00, 45769.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJvLxFWrohDD",
        "colab_type": "code",
        "outputId": "0f95970b-5db8-4ffd-ab3e-aea68879f4ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        }
      },
      "cell_type": "code",
      "source": [
        "# callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 3, mode= 'auto'),\n",
        "    keras.callbacks.ModelCheckpoint('data/descriptor_model.weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=50, callbacks=callbacks,\n",
        "                                              verbose=1, validation_data=val_generator)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "199/200 [============================>.] - ETA: 1s - loss: 0.2480"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:05<00:00, 17999.98it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 43806.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 227s 1s/step - loss: 0.2477 - val_loss: 0.2478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.24775, saving model to data/descriptor_model.weights.01-0.25.hdf5\n",
            "Epoch 2/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:02<00:00, 35097.82it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 45734.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 199s 995ms/step - loss: 0.1719 - val_loss: 0.2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00002: val_loss improved from 0.24775 to 0.20209, saving model to data/descriptor_model.weights.02-0.20.hdf5\n",
            "Epoch 3/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1568"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:03<00:00, 32878.77it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 45925.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 199s 994ms/step - loss: 0.1566 - val_loss: 0.1835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00003: val_loss improved from 0.20209 to 0.18353, saving model to data/descriptor_model.weights.03-0.18.hdf5\n",
            "Epoch 4/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1457"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:02<00:00, 34668.26it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 47147.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 199s 994ms/step - loss: 0.1456 - val_loss: 0.1858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00004: val_loss did not improve from 0.18353\n",
            "Epoch 5/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:03<00:00, 33017.62it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 44863.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 199s 995ms/step - loss: 0.1384 - val_loss: 0.1681\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.18353 to 0.16806, saving model to data/descriptor_model.weights.05-0.17.hdf5\n",
            "Epoch 6/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1320"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:03<00:00, 32334.19it/s]\n",
            " 89%|████████▊ | 8854/10000 [00:00<00:00, 41224.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 200s 1s/step - loss: 0.1320 - val_loss: 0.1572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10000/10000 [00:00<00:00, 30192.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00006: val_loss improved from 0.16806 to 0.15718, saving model to data/descriptor_model.weights.06-0.16.hdf5\n",
            "Epoch 7/50\n",
            " 33/200 [===>..........................] - ETA: 2:41 - loss: 0.1317"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5c144faeee73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=50, callbacks=callbacks,\n\u001b[0;32m----> 7\u001b[0;31m                                               verbose=1, validation_data=val_generator)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "NHaLjwg2TbWI",
        "colab_type": "code",
        "outputId": "b3896616-367a-43b6-c6b8-917138847bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(descriptor_history.history['loss'])\n",
        "plt.plot(descriptor_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4ea0b46f7391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'descriptor_history' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iYzV5z2BBkN5",
        "colab_type": "code",
        "outputId": "0281138a-71cd-4cd1-a90d-022b87c56ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from get_data import get_data\n",
        "%cd hpatches-benchmark\n",
        "!git pull \n",
        "%cd ..\n",
        "!mkdir -p results\n",
        "\n",
        "generate_desc_csv(descriptor_model, seqs_test, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deepLearning/hpatches-benchmark\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/AlexMontgomerie/deepLearning\n",
            "   05a47de..b44ace7  master     -> origin/master\n",
            "Updating 05a47de..b44ace7\n",
            "Fast-forward\n",
            " full_net_regularised.ipynb | 762 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 1 file changed, 762 insertions(+)\n",
            " create mode 100644 full_net_regularised.ipynb\n",
            "/content/deepLearning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 3/40 [00:40<09:50, 15.96s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}