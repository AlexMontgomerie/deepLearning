{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMontgomerie/deepLearning/blob/master/full_net_regularised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tSMVa8-rncVS",
        "colab_type": "code",
        "outputId": "15132697-23e8-4453-ea81-8b95e877fbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!rm -rf deepLearning\n",
        "!git clone https://github.com/AlexMontgomerie/deepLearning\n",
        "%cd deepLearning\n",
        "!git pull origin master\n",
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deepLearning'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 405 (delta 85), reused 58 (delta 25), pack-reused 245\u001b[K\n",
            "Receiving objects: 100% (405/405), 152.39 MiB | 18.09 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n",
            "Checking out files: 100% (109/109), done.\n",
            "/content/deepLearning\n",
            "From https://github.com/AlexMontgomerie/deepLearning\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "('RAM Free: 12.9 GB', ' | Proc size: 153.1 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWqnNDkSnqfz",
        "colab_type": "code",
        "outputId": "28d92389-867b-45ec-9ead-43a72cdfebea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "cell_type": "code",
      "source": [
        "from common import *\n",
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--2019-03-21 05:28:33--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 107.152.25.197, 107.152.24.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|107.152.25.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-21 05:28:33--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-21 05:28:33--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 107.152.26.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|107.152.26.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!SJXZIkNDho5ZOtrhQrOjfwbhWgkquOE76hefyTTQ3N2es0MOaMms8aJTqA0NzYayTlhWgm2pu_IDqRv9lv6JKgR9y5n_jXJvCXoCx-C7Ng46nevf1wKtM45_2NzRO-n3IJkbn4_S8zV73Hl6RgDVPq7OVvXtNQjwG794GwIxXzoLj0hBo35ZFt95xdXDl83KYrY1bQAd_Bzrq5GA4jS663RsBghO7fhq-PbahPKeBS7IlvZfYtFk1hIHwfYs5-zZcZQfmcCh_JBesmWw11IbWZIAmA9fNRXhIDqMco_3qKUqlzkztkrvUhmRKF9Dhh41oICu4zoLX2M5Ne_v-MZjJcawygKs6DIFFNFIcM75vDRC4Di37WmqCEtGknpz40LXJemTxqFhw-XX79ywXhQ6nLiuPvJlwna0NmGKJhqIq2IpH3nDYoPnnYlSc2V_Zw8M6Fg_5YEFZZDsEJVs-McDXi9wlb1DCoezsD9zNJZRuCAJZTHmVWB9j0otl6ScD4znSajY7IeXRnsY9pMs5uKwXICv3BKceBs-eNlpTLRIxRv8yuFfnOWmnI4L2hCq0RxOqlU1zWkAsWf2Wyh_ZUDFW2AdmaVJzK2OFmZFs0a_cNVFJOzW5pEHNUohOkafA198GVb_yXP9CYly0DdP9FD4w3qr5hbqsg3icrlMhnC69ep4crx6k1_5thk35b77CoQBDT4ARju7HE8_C_Xc-AVoInvpJ5NFrp2jr_NTF8og0xS_TPhsT7u7hRilQTt7z815k-6ThElBGnixT7iiojqlYQEWFaS_NzviDKt9Wys9sod1q2gwQTPIZWd_zGc_7XkYTRtJe5jfIraD0uty3C8yOz9jl7ofOPOEIt8uMHxtElxFwnrfMHVmGpFR6uZNJ57iUxWBOcfDC49_QIGKLkQQbRawmtkShXO07zPkTApR5ezCTBzevI2g5JOSiQFLp_7oj0-DiT01RHbY6nI0PC78LMdImL2AWvK8XIPmPHA02xJDV497LMy1xci5FFKTO_39RgRLCXmTMomPUy7Gl9E2kpJqmGoUny0vjJvMkUqYUrx4T8PfRu5diei-4Xn95hK4K4VBrcejXb2JceJmkVl1uQdZJxffELwkI3uVZCiSRTxqpaWP2pMWSm1-7d16_Mwqxq2VyfNW0n8aW7g_Z_xOae7NZhJe5H1dUGSlHV6UAR73DB6s4gqDu6y73cSE4_uBHsek0UmAx2w9uupMYJwZOmsgJdwi7FyM9ZtHXu7pyWHDEBkJ7kfGOUnSol8nzzY1TLnbPnLjXnUeZWh3IUH_UGwgK6LfQrc91PkwCgtuPCqApNPDugWtMYqubr_JmAs5YtuNOOjV-hQA8d4Ez4ddr7OhEj8iUXm7JbBP7iP8u6EMqgpZDOGiEHghLGCez86iyEwbuNqJOVx2uxL5_rMQW4_BB5f4Bd59ZhhMwJpZ4zvs8uCuUYW-krjtLuyJSUHg2jrGqg9-ua__QcXGtHz5OoC72LVvdUEg/download [following]\n",
            "--2019-03-21 05:28:34--  https://public.boxcloud.com/d/1/b1!SJXZIkNDho5ZOtrhQrOjfwbhWgkquOE76hefyTTQ3N2es0MOaMms8aJTqA0NzYayTlhWgm2pu_IDqRv9lv6JKgR9y5n_jXJvCXoCx-C7Ng46nevf1wKtM45_2NzRO-n3IJkbn4_S8zV73Hl6RgDVPq7OVvXtNQjwG794GwIxXzoLj0hBo35ZFt95xdXDl83KYrY1bQAd_Bzrq5GA4jS663RsBghO7fhq-PbahPKeBS7IlvZfYtFk1hIHwfYs5-zZcZQfmcCh_JBesmWw11IbWZIAmA9fNRXhIDqMco_3qKUqlzkztkrvUhmRKF9Dhh41oICu4zoLX2M5Ne_v-MZjJcawygKs6DIFFNFIcM75vDRC4Di37WmqCEtGknpz40LXJemTxqFhw-XX79ywXhQ6nLiuPvJlwna0NmGKJhqIq2IpH3nDYoPnnYlSc2V_Zw8M6Fg_5YEFZZDsEJVs-McDXi9wlb1DCoezsD9zNJZRuCAJZTHmVWB9j0otl6ScD4znSajY7IeXRnsY9pMs5uKwXICv3BKceBs-eNlpTLRIxRv8yuFfnOWmnI4L2hCq0RxOqlU1zWkAsWf2Wyh_ZUDFW2AdmaVJzK2OFmZFs0a_cNVFJOzW5pEHNUohOkafA198GVb_yXP9CYly0DdP9FD4w3qr5hbqsg3icrlMhnC69ep4crx6k1_5thk35b77CoQBDT4ARju7HE8_C_Xc-AVoInvpJ5NFrp2jr_NTF8og0xS_TPhsT7u7hRilQTt7z815k-6ThElBGnixT7iiojqlYQEWFaS_NzviDKt9Wys9sod1q2gwQTPIZWd_zGc_7XkYTRtJe5jfIraD0uty3C8yOz9jl7ofOPOEIt8uMHxtElxFwnrfMHVmGpFR6uZNJ57iUxWBOcfDC49_QIGKLkQQbRawmtkShXO07zPkTApR5ezCTBzevI2g5JOSiQFLp_7oj0-DiT01RHbY6nI0PC78LMdImL2AWvK8XIPmPHA02xJDV497LMy1xci5FFKTO_39RgRLCXmTMomPUy7Gl9E2kpJqmGoUny0vjJvMkUqYUrx4T8PfRu5diei-4Xn95hK4K4VBrcejXb2JceJmkVl1uQdZJxffELwkI3uVZCiSRTxqpaWP2pMWSm1-7d16_Mwqxq2VyfNW0n8aW7g_Z_xOae7NZhJe5H1dUGSlHV6UAR73DB6s4gqDu6y73cSE4_uBHsek0UmAx2w9uupMYJwZOmsgJdwi7FyM9ZtHXu7pyWHDEBkJ7kfGOUnSol8nzzY1TLnbPnLjXnUeZWh3IUH_UGwgK6LfQrc91PkwCgtuPCqApNPDugWtMYqubr_JmAs5YtuNOOjV-hQA8d4Ez4ddr7OhEj8iUXm7JbBP7iP8u6EMqgpZDOGiEHghLGCez86iyEwbuNqJOVx2uxL5_rMQW4_BB5f4Bd59ZhhMwJpZ4zvs8uCuUYW-krjtLuyJSUHg2jrGqg9-ua__QcXGtHz5OoC72LVvdUEg/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.25.200, 107.152.24.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.25.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip    85%[================>   ]   3.24G  24.6MB/s    eta 25s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0YF2io-cnthT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Input, UpSampling2D, concatenate, Subtract\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from read_data import HPatches, HPatchesRegularised, DataGeneratorDescRegularised, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, STNHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "import matplotlib.pyplot as plt\n",
        "from layers import BilinearInterpolation\n",
        "from keras.layers import Layer, Lambda\n",
        "\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "def run_sobel(image):\n",
        "  return tf.image.sobel_edges(image)[:,:,:,0]\n",
        "      \n",
        "\n",
        "def get_initial_weights(output_size):\n",
        "    b = np.zeros((2, 3), dtype='float32')\n",
        "    b[0, 0] = 1\n",
        "    b[1, 1] = 1\n",
        "    W = np.zeros((output_size, 6), dtype='float32')\n",
        "    weights = [W, b.flatten()]\n",
        "    return weights\n",
        "  \n",
        "def get_full_model(shape,stn_init=None):  \n",
        "    #shape = (32, 32, 1)\n",
        "\n",
        "    init_weights = keras.initializers.he_normal()\n",
        "    # input \n",
        "    inputs = Input(shape)\n",
        "    \n",
        "    # denoise network\n",
        "    depth1  = 32\n",
        "    conv1_1 = Conv2D(depth1, 1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "\n",
        "    # convolution layers\n",
        "    conv1_2  = Conv2D(depth1, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_3  = Conv2D(depth1, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_5  = Conv2D(depth1, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_7  = Conv2D(depth1, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_9  = Conv2D(depth1, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_11 = Conv2D(depth1, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "\n",
        "    # network\n",
        "    net1 = Subtract()([conv1_1, conv1_2])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_3])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_5])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_7])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_9])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_11])\n",
        "    net1 = BatchNormalization()(net1)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth2 = 16\n",
        "    conv2_1  = Conv2D(depth2, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv2_2  = Conv2D(depth2, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_3  = Conv2D(depth2, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_5  = Conv2D(depth2, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_7  = Conv2D(depth2, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_9  = Conv2D(depth2, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_11 = Conv2D(depth2, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "\n",
        "    # network\n",
        "    net2 = Subtract()([conv2_1, conv2_2])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_3])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_5])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_7])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_9])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_11])\n",
        "    net2 = BatchNormalization()(net2)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth3 = 8\n",
        "    conv3_1  = Conv2D(depth3, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv3_2  = Conv2D(depth3, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_3  = Conv2D(depth3, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_5  = Conv2D(depth3, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_7  = Conv2D(depth3, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_9  = Conv2D(depth3, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_11 = Conv2D(depth3, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "\n",
        "    # network\n",
        "    net3 = Subtract()([conv3_1, conv3_2])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_3])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_5])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_7])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_9])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_11])\n",
        "    net3 = BatchNormalization()(net3)  \n",
        "\n",
        "    #net = Conv2D(1, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(net3)\n",
        "  \n",
        "    \n",
        "    # stn network    \n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(net3)\n",
        "    locnet = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = Flatten()(locnet)\n",
        "    locnet = Dense(100)(locnet)\n",
        "    locnet = Activation('sigmoid')(locnet)\n",
        "    weights = get_initial_weights(100)\n",
        "    locnet = Dense(6, weights=weights)(locnet)\n",
        "    stn    = BilinearInterpolation(shape[:-1])([net3, locnet])\n",
        "    \n",
        "    if stn_init:\n",
        "      stn.set_weights(stn_init.get_weights())\n",
        "      \n",
        "    # sobel\n",
        "    sobel = Lambda(run_sobel)(net3)\n",
        "    \n",
        "    # features in\n",
        "    l2net = concatenate([ net3, stn , sobel ], axis = -1)\n",
        "    \n",
        "    # L2 Net    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 8, padding='valid', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "\n",
        "    l2net = Reshape((128,))(l2net)\n",
        "  \n",
        "    l2net = Model(inputs = inputs, outputs = l2net)\n",
        "    \n",
        "    descriptor_model = Sequential()\n",
        "    descriptor_model.add(l2net)\n",
        "    \n",
        "    return descriptor_model\n",
        "\n",
        "    \n",
        "  \n",
        "from keras.layers import Lambda\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "sgd = keras.optimizers.nadam()\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N6a8ibZjnR8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "alphaIn = Input(shape=(1,), name='alpha')\n",
        "\n",
        "def triplet_loss_regularised(x):  \n",
        "  a, p, n, _alpha = x\n",
        "\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "\n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha[0]), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss_regularised)([ea, ep, en, alphaIn])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn, alphaIn], outputs=loss)\n",
        "sgd = keras.optimizers.SGD(lr=0.1)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjpyrKsnnbje",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]\n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs))\n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatchesRegularised(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000, batch_size=50)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJvLxFWrohDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# callbacks\n",
        "callbacks = [\n",
        "    #keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 3, mode= 'auto'),\n",
        "    keras.callbacks.ModelCheckpoint('data/descriptor_model.weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=200, callbacks=callbacks,\n",
        "                                              verbose=1, validation_data=val_generator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHaLjwg2TbWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(descriptor_history.history['loss'])\n",
        "plt.plot(descriptor_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "np.save('data/net_loss.npy', np.array(descriptor_history.history['loss']))\n",
        "np.save('data/net_val_loss.npy', np.array(descriptor_history.history['val_loss']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYzV5z2BBkN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from get_data import get_data\n",
        "%cd hpatches-benchmark\n",
        "!git pull \n",
        "%cd ..\n",
        "!mkdir -p results\n",
        "\n",
        "generate_desc_csv(descriptor_model, seqs_test, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}