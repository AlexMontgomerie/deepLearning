{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMontgomerie/deepLearning/blob/master/denoise_additive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hjTIqmNec40a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bb9c4d40-fc94-4447-f58f-71e9604e2442"
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!rm -rf deepLearning\n",
        "!git clone https://github.com/AlexMontgomerie/deepLearning\n",
        "%cd deepLearning\n",
        "!git pull origin master\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'deepLearning' already exists and is not an empty directory.\n",
            "/content/deepLearning\n",
            "From https://github.com/AlexMontgomerie/deepLearning\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "('RAM Free: 12.9 GB', ' | Proc size: 150.1 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lPQ7XCWvdZ6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85c10cf2-3e7f-4e09-d63f-217aa2b64020"
      },
      "cell_type": "code",
      "source": [
        "from common import *\n",
        "!./setup.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tKQ2MRffaFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa154396-1e83-4ebd-c701-e8c5b0821ec1"
      },
      "cell_type": "code",
      "source": [
        "from denoise_network import *\n",
        "from get_data import get_data\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "seqs_train, seqs_test = get_data()\n",
        "\n",
        "# get traning data\n",
        "\n",
        "denoise_generator     = DenoiseHPatches(seqs_train, batch_size=500)\n",
        "denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=500)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 76/76 [01:11<00:00,  1.32it/s]\n",
            "100%|██████████| 40/40 [00:43<00:00,  1.31s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-BV5WKqNdfgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Input, UpSampling2D, concatenate, Subtract\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_denoise_model_additive(shape):\n",
        "\n",
        "  inputs  = Input(shape)\n",
        "  depth1  = 32\n",
        "  conv1_1 = Conv2D(depth1, 1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  \n",
        "  # convolution layers\n",
        "  conv1_2  = Conv2D(depth1, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  conv1_3  = Conv2D(depth1, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  conv1_5  = Conv2D(depth1, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  conv1_7  = Conv2D(depth1, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  conv1_9  = Conv2D(depth1, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  conv1_11 = Conv2D(depth1, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "  \n",
        "  # network\n",
        "  net1 = Subtract()([conv1_1, conv1_2])\n",
        "  net1 = BatchNormalization()(net1)\n",
        "  net1 = Subtract()([conv1_1, conv1_3])\n",
        "  net1 = BatchNormalization()(net1)\n",
        "  net1 = Subtract()([conv1_1, conv1_5])\n",
        "  net1 = BatchNormalization()(net1)\n",
        "  net1 = Subtract()([conv1_1, conv1_7])\n",
        "  net1 = BatchNormalization()(net1)\n",
        "  net1 = Subtract()([conv1_1, conv1_9])\n",
        "  net1 = BatchNormalization()(net1)\n",
        "  net1 = Subtract()([conv1_1, conv1_11])\n",
        "  net1 = BatchNormalization()(net1)  \n",
        "  \n",
        "  # convolution layers\n",
        "  depth2 = 16\n",
        "  conv2_1  = Conv2D(depth2, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "  conv2_2  = Conv2D(depth2, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  conv2_3  = Conv2D(depth2, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  conv2_5  = Conv2D(depth2, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  conv2_7  = Conv2D(depth2, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  conv2_9  = Conv2D(depth2, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  conv2_11 = Conv2D(depth2, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "  \n",
        "  # network\n",
        "  net2 = Subtract()([conv2_1, conv2_2])\n",
        "  net2 = BatchNormalization()(net2)\n",
        "  net2 = Subtract()([conv2_1, conv2_3])\n",
        "  net2 = BatchNormalization()(net2)\n",
        "  net2 = Subtract()([conv2_1, conv2_5])\n",
        "  net2 = BatchNormalization()(net2)\n",
        "  net2 = Subtract()([conv2_1, conv2_7])\n",
        "  net2 = BatchNormalization()(net2)\n",
        "  net2 = Subtract()([conv2_1, conv2_9])\n",
        "  net2 = BatchNormalization()(net2)\n",
        "  net2 = Subtract()([conv2_1, conv2_11])\n",
        "  net2 = BatchNormalization()(net2)  \n",
        "  \n",
        "  # convolution layers\n",
        "  depth3 = 8\n",
        "  conv3_1  = Conv2D(depth3, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "  conv3_2  = Conv2D(depth3, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  conv3_3  = Conv2D(depth3, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  conv3_5  = Conv2D(depth3, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  conv3_7  = Conv2D(depth3, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  conv3_9  = Conv2D(depth3, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  conv3_11 = Conv2D(depth3, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "  \n",
        "  # network\n",
        "  net3 = Subtract()([conv3_1, conv2_2])\n",
        "  net3 = BatchNormalization()(net3)\n",
        "  net3 = Subtract()([conv3_1, conv3_3])\n",
        "  net3 = BatchNormalization()(net3)\n",
        "  net3 = Subtract()([conv3_1, conv3_5])\n",
        "  net3 = BatchNormalization()(net3)\n",
        "  net3 = Subtract()([conv3_1, conv3_7])\n",
        "  net3 = BatchNormalization()(net3)\n",
        "  net3 = Subtract()([conv3_1, conv3_9])\n",
        "  net3 = BatchNormalization()(net3)\n",
        "  net3 = Subtract()([conv3_1, conv3_11])\n",
        "  net3 = BatchNormalization()(net3)  \n",
        "  \n",
        "  net = Conv2D(1, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(net3)\n",
        "  \n",
        "  return Model(inputs = inputs, outputs = net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0wHO0InfUpf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2023
        },
        "outputId": "768959e6-a2f3-440d-ed2f-53bf55075a30"
      },
      "cell_type": "code",
      "source": [
        "# optimiser\n",
        "opt  = opt = keras.optimizers.nadam()\n",
        "\n",
        "# get model\n",
        "shape = (32, 32, 1)\n",
        "denoise_model = get_denoise_model_additive(shape)\n",
        "\n",
        "# callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 3, mode= 'auto'),\n",
        "    keras.callbacks.ModelCheckpoint('data/denoise_model.weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['accuracy'])\n",
        "denoise_history = denoise_model.fit_generator(generator=denoise_generator, epochs=EPOCHS, callbacks=callbacks,\n",
        "                                              verbose=1, validation_data=denoise_generator_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3117/3117 [==============================] - 540s 173ms/step - loss: 22.8066 - acc: 0.0411 - val_loss: 6.1367 - val_acc: 0.0613\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.13668, saving model to data/denoise_model.weights.01-6.14.hdf5\n",
            "Epoch 2/50\n",
            "3117/3117 [==============================] - 536s 172ms/step - loss: 6.3540 - acc: 0.0590 - val_loss: 5.8579 - val_acc: 0.0654\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.13668 to 5.85790, saving model to data/denoise_model.weights.02-5.86.hdf5\n",
            "Epoch 3/50\n",
            "3117/3117 [==============================] - 534s 171ms/step - loss: 6.1618 - acc: 0.0610 - val_loss: 5.7704 - val_acc: 0.0675\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.85790 to 5.77045, saving model to data/denoise_model.weights.03-5.77.hdf5\n",
            "Epoch 4/50\n",
            "3117/3117 [==============================] - 534s 171ms/step - loss: 5.9290 - acc: 0.0647 - val_loss: 5.5044 - val_acc: 0.0735\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.77045 to 5.50442, saving model to data/denoise_model.weights.04-5.50.hdf5\n",
            "Epoch 5/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.7835 - acc: 0.0671 - val_loss: 5.7541 - val_acc: 0.0671\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 5.50442\n",
            "Epoch 6/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.8233 - acc: 0.0666 - val_loss: 5.3572 - val_acc: 0.0773\n",
            "\n",
            "Epoch 00006: val_loss improved from 5.50442 to 5.35725, saving model to data/denoise_model.weights.06-5.36.hdf5\n",
            "Epoch 7/50\n",
            "3117/3117 [==============================] - 538s 172ms/step - loss: 5.6606 - acc: 0.0694 - val_loss: 5.4076 - val_acc: 0.0747\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 5.35725\n",
            "Epoch 8/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.7094 - acc: 0.0685 - val_loss: 5.7208 - val_acc: 0.0687\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 5.35725\n",
            "Epoch 9/50\n",
            "3117/3117 [==============================] - 535s 172ms/step - loss: 5.6041 - acc: 0.0704 - val_loss: 5.4742 - val_acc: 0.0703\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 5.35725\n",
            "Epoch 10/50\n",
            "3117/3117 [==============================] - 532s 171ms/step - loss: 5.6475 - acc: 0.0698 - val_loss: 5.3111 - val_acc: 0.0793\n",
            "\n",
            "Epoch 00010: val_loss improved from 5.35725 to 5.31110, saving model to data/denoise_model.weights.10-5.31.hdf5\n",
            "Epoch 11/50\n",
            "3117/3117 [==============================] - 530s 170ms/step - loss: 5.5463 - acc: 0.0716 - val_loss: 5.7784 - val_acc: 0.0691\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 5.31110\n",
            "Epoch 12/50\n",
            "3117/3117 [==============================] - 532s 171ms/step - loss: 5.7018 - acc: 0.0688 - val_loss: 5.6256 - val_acc: 0.0708\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 5.31110\n",
            "Epoch 13/50\n",
            "3117/3117 [==============================] - 534s 171ms/step - loss: 5.5312 - acc: 0.0721 - val_loss: 5.4379 - val_acc: 0.0685\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 5.31110\n",
            "Epoch 14/50\n",
            "3117/3117 [==============================] - 532s 171ms/step - loss: 5.5005 - acc: 0.0728 - val_loss: 5.7472 - val_acc: 0.0689\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 5.31110\n",
            "Epoch 15/50\n",
            "3117/3117 [==============================] - 530s 170ms/step - loss: 5.4975 - acc: 0.0730 - val_loss: 5.2283 - val_acc: 0.0825\n",
            "\n",
            "Epoch 00015: val_loss improved from 5.31110 to 5.22829, saving model to data/denoise_model.weights.15-5.23.hdf5\n",
            "Epoch 16/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.6107 - acc: 0.0708 - val_loss: 5.2095 - val_acc: 0.0834\n",
            "\n",
            "Epoch 00016: val_loss improved from 5.22829 to 5.20954, saving model to data/denoise_model.weights.16-5.21.hdf5\n",
            "Epoch 17/50\n",
            "3117/3117 [==============================] - 536s 172ms/step - loss: 5.4804 - acc: 0.0735 - val_loss: 5.2846 - val_acc: 0.0757\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.20954\n",
            "Epoch 18/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.4709 - acc: 0.0737 - val_loss: 5.4320 - val_acc: 0.0844\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.20954\n",
            "Epoch 19/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.6067 - acc: 0.0712 - val_loss: 5.6261 - val_acc: 0.0696\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 5.20954\n",
            "Epoch 20/50\n",
            "3117/3117 [==============================] - 538s 172ms/step - loss: 5.5425 - acc: 0.0721 - val_loss: 5.3665 - val_acc: 0.0849\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.20954\n",
            "Epoch 21/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.4755 - acc: 0.0737 - val_loss: 5.2037 - val_acc: 0.0877\n",
            "\n",
            "Epoch 00021: val_loss improved from 5.20954 to 5.20373, saving model to data/denoise_model.weights.21-5.20.hdf5\n",
            "Epoch 22/50\n",
            "3117/3117 [==============================] - 537s 172ms/step - loss: 5.4280 - acc: 0.0751 - val_loss: 5.2042 - val_acc: 0.0858\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 5.20373\n",
            "Epoch 23/50\n",
            "3117/3117 [==============================] - 535s 172ms/step - loss: 5.4160 - acc: 0.0758 - val_loss: 5.2260 - val_acc: 0.0881\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 5.20373\n",
            "Epoch 24/50\n",
            "3117/3117 [==============================] - 533s 171ms/step - loss: 5.4098 - acc: 0.0757 - val_loss: 5.2886 - val_acc: 0.0906\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 5.20373\n",
            "Epoch 25/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.4297 - acc: 0.0755 - val_loss: 5.1673 - val_acc: 0.0833\n",
            "\n",
            "Epoch 00025: val_loss improved from 5.20373 to 5.16727, saving model to data/denoise_model.weights.25-5.17.hdf5\n",
            "Epoch 26/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.4330 - acc: 0.0759 - val_loss: 5.2297 - val_acc: 0.0904\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 5.16727\n",
            "Epoch 27/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.3907 - acc: 0.0768 - val_loss: 5.1916 - val_acc: 0.0888\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 5.16727\n",
            "Epoch 28/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.3873 - acc: 0.0772 - val_loss: 5.1324 - val_acc: 0.0899\n",
            "\n",
            "Epoch 00028: val_loss improved from 5.16727 to 5.13242, saving model to data/denoise_model.weights.28-5.13.hdf5\n",
            "Epoch 29/50\n",
            "3117/3117 [==============================] - 531s 170ms/step - loss: 5.3773 - acc: 0.0779 - val_loss: 5.1669 - val_acc: 0.0963\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 5.13242\n",
            "Epoch 30/50\n",
            " 501/3117 [===>..........................] - ETA: 6:12 - loss: 5.3671 - acc: 0.0784"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}