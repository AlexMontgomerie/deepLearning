{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMontgomerie/deepLearning/blob/master/improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y_vrxNdjvC3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Improved Network"
      ]
    },
    {
      "metadata": {
        "id": "4U5bWoRnvG8m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initial Setup\n",
        "\n",
        "__IMPORTANT:__\n",
        "Before running this code, make sure to move all the files below into the `/content` folder.\n",
        "\n",
        " - common.py\n",
        " - read_data.py\n",
        " - layers.py\n",
        " - utils.py\n",
        " - setup.sh\n",
        " \n",
        "\n",
        "The following code block installs relevant python packages and setups the GPU hardware for use with keras."
      ]
    },
    {
      "metadata": {
        "id": "tSMVa8-rncVS",
        "colab_type": "code",
        "outputId": "84c93521-4d24-4346-a7d3-bd08d7d2c761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!cp ../common.py /content\n",
        "#!cp ../read_data.py /content\n",
        "#!cp ../layers.py /content\n",
        "#!cp ../utils.py /content\n",
        "#!cp ../setup.sh /content\n",
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "('RAM Free: 12.9 GB', ' | Proc size: 154.3 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lD1BOsrtvTxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Dataset\n",
        "\n",
        "The following script downloads the dataset into the environment."
      ]
    },
    {
      "metadata": {
        "id": "eWqnNDkSnqfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from common import *\n",
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfKKVCwawyWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Network\n",
        "\n",
        "The following code block details the finalised network which is used. There is a fixed seed which is used for consistency while evaluating, however can easily be removed. \n",
        "\n",
        "The network is as outlined in the report, with the denoising, feature extraction and descriptors combined into one model."
      ]
    },
    {
      "metadata": {
        "id": "0YF2io-cnthT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Input, UpSampling2D, concatenate, Subtract\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from read_data import HPatches, HPatchesRegularised, DataGeneratorDescRegularised, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, STNHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "import matplotlib.pyplot as plt\n",
        "from layers import BilinearInterpolation\n",
        "from keras.layers import Layer, Lambda\n",
        "\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "# sobel filter\n",
        "def run_sobel(image):\n",
        "  return tf.image.sobel_edges(image)[:,:,:,0]\n",
        "      \n",
        "# initial weights for the STN network\n",
        "def get_initial_weights(output_size):\n",
        "    b = np.zeros((2, 3), dtype='float32')\n",
        "    b[0, 0] = 1\n",
        "    b[1, 1] = 1\n",
        "    W = np.zeros((output_size, 6), dtype='float32')\n",
        "    weights = [W, b.flatten()]\n",
        "    return weights\n",
        "  \n",
        "# Description of the complete model\n",
        "def get_full_model(shape,stn_init=None):  \n",
        "\n",
        "    init_weights = keras.initializers.he_normal()\n",
        "    \n",
        "    # input \n",
        "    inputs = Input(shape)\n",
        "    \n",
        "    # denoise network\n",
        "    depth1  = 32\n",
        "    conv1_1 = Conv2D(depth1, 1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "\n",
        "    # convolution layers\n",
        "    conv1_2  = Conv2D(depth1, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_3  = Conv2D(depth1, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_5  = Conv2D(depth1, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_7  = Conv2D(depth1, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_9  = Conv2D(depth1, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_11 = Conv2D(depth1, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "\n",
        "    # network\n",
        "    net1 = Subtract()([conv1_1, conv1_2])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_3])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_5])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_7])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_9])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_11])\n",
        "    net1 = BatchNormalization()(net1)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth2 = 16\n",
        "    conv2_1  = Conv2D(depth2, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv2_2  = Conv2D(depth2, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_3  = Conv2D(depth2, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_5  = Conv2D(depth2, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_7  = Conv2D(depth2, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_9  = Conv2D(depth2, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_11 = Conv2D(depth2, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "\n",
        "    # network\n",
        "    net2 = Subtract()([conv2_1, conv2_2])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_3])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_5])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_7])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_9])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_11])\n",
        "    net2 = BatchNormalization()(net2)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth3 = 8\n",
        "    conv3_1  = Conv2D(depth3, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv3_2  = Conv2D(depth3, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_3  = Conv2D(depth3, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_5  = Conv2D(depth3, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_7  = Conv2D(depth3, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_9  = Conv2D(depth3, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_11 = Conv2D(depth3, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "\n",
        "    # network\n",
        "    net3 = Subtract()([conv3_1, conv3_2])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_3])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_5])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_7])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_9])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_11])\n",
        "    net3 = BatchNormalization()(net3)  \n",
        "\n",
        "  \n",
        "    # stn network    \n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(net3)\n",
        "    locnet = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = Flatten()(locnet)\n",
        "    locnet = Dense(100)(locnet)\n",
        "    locnet = Activation('sigmoid')(locnet)\n",
        "    weights = get_initial_weights(100)\n",
        "    locnet = Dense(6, weights=weights)(locnet)\n",
        "    stn    = BilinearInterpolation(shape[:-1])([net3, locnet])\n",
        "      \n",
        "    # sobel\n",
        "    sobel = Lambda(run_sobel)(net3)\n",
        "    \n",
        "    # features in\n",
        "    l2net = concatenate([ net3, stn , sobel ], axis = -1)\n",
        "    \n",
        "    # L2 Net    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 8, padding='valid', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "\n",
        "    l2net = Reshape((128,))(l2net)\n",
        "  \n",
        "    l2net = Model(inputs = inputs, outputs = l2net)\n",
        "    \n",
        "    descriptor_model = Sequential()\n",
        "    descriptor_model.add(l2net)\n",
        "    \n",
        "    return descriptor_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1QstTDaxTEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Loss function\n",
        "\n",
        "The code block below outlines the loss function used for training. This is similar to the triplet loss function seen in the baseline, however the alpha variable is passed as a parameter to the loss function.\n",
        "\n",
        "As can be seen, an NAdam optimiser os used for the final training.\n"
      ]
    },
    {
      "metadata": {
        "id": "N6a8ibZjnR8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "alphaIn = Input(shape=(1,), name='alpha')\n",
        "\n",
        "def triplet_loss_regularised(x):  \n",
        "  a, p, n, _alpha = x\n",
        "\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "\n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha[0]), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss_regularised)([ea, ep, en, alphaIn])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn, alphaIn], outputs=loss)\n",
        "opt = keras.optimizers.SGD(lr=0.1)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxiKk5K4xzvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training and Validation Sets\n",
        "\n",
        "The same `a` split is used for training as with the baseline, so training and test images are the same. Instead of the HPatches data generator, HPatchesRegularised uses the more regularised loss function documented in the report. It contains an anchor, positive and negative image, as well as alpha value based on the images. A regularisation value of 0.75 is chosen for negative pairs of the same image. Batch size is of 50."
      ]
    },
    {
      "metadata": {
        "id": "yjpyrKsnnbje",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]\n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs))\n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatchesRegularised(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000, batch_size=50)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yl5gF0YmznM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "The network is trained over 100 epochs, and the models with the best validation score are kept every epoch."
      ]
    },
    {
      "metadata": {
        "id": "pJvLxFWrohDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint('data/descriptor_model.weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=100, callbacks=callbacks,\n",
        "                                              verbose=1, validation_data=val_generator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TdIGhkvi1o66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Plot\n",
        "\n",
        "Quick plot of training."
      ]
    },
    {
      "metadata": {
        "id": "NHaLjwg2TbWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(descriptor_history.history['loss'])\n",
        "plt.plot(descriptor_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "np.save('data/net_loss.npy', np.array(descriptor_history.history['loss']))\n",
        "np.save('data/net_val_loss.npy', np.array(descriptor_history.history['val_loss']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eN47Yzva1xt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Benchmark\n",
        "\n",
        "The final network is benchmarked using the HPatches benchmark."
      ]
    },
    {
      "metadata": {
        "id": "iYzV5z2BBkN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from get_data import get_data\n",
        "%cd hpatches-benchmark\n",
        "!git pull \n",
        "%cd ..\n",
        "!mkdir -p results\n",
        "\n",
        "generate_desc_csv(descriptor_model, seqs_test, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}