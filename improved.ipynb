{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMontgomerie/deepLearning/blob/master/improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y_vrxNdjvC3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Improved Network"
      ]
    },
    {
      "metadata": {
        "id": "4U5bWoRnvG8m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initial Setup\n",
        "\n",
        "__IMPORTANT:__\n",
        "Before running this code, make sure to move all the files below into the `/content` folder.\n",
        "\n",
        " - common.py\n",
        " - read_data.py\n",
        " - layers.py\n",
        " - utils.py\n",
        " - setup.sh\n",
        " - splits.json\n",
        "\n",
        "The following code block installs relevant python packages and setups the GPU hardware for use with keras."
      ]
    },
    {
      "metadata": {
        "id": "tSMVa8-rncVS",
        "colab_type": "code",
        "outputId": "bfb1c6de-a38d-4bc9-f1d6-f9c86ec29905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!cp ../common.py /content\n",
        "#!cp ../read_data.py /content\n",
        "#!cp ../layers.py /content\n",
        "#!cp ../utils.py /content\n",
        "#!cp ../setup.sh /content\n",
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n",
            "('RAM Free: 12.9 GB', ' | Proc size: 154.4 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lD1BOsrtvTxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Dataset\n",
        "\n",
        "The following script downloads the dataset into the environment."
      ]
    },
    {
      "metadata": {
        "id": "eWqnNDkSnqfz",
        "colab_type": "code",
        "outputId": "51893098-0506-48b4-8ee7-df621dd89cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "cell_type": "code",
      "source": [
        "from common import *\n",
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--2019-03-21 06:11:41--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 107.152.26.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|107.152.26.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-21 06:11:41--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-21 06:11:41--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 107.152.27.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|107.152.27.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!75HpBOZmq_Lp8HQjyzm9H0Zx1Zw5_wzymgFBDYuhB2konfXiQm-TMyKWk7x-9yWsiYHKA4pCdd9UdSiOzyhWSt7sDl13NjB9_FAyhyPXBYEiWyrA68NQpSGrXHbzOu3dfDazSzJRIQEBJ5JNpi4PUdHAiwo5o1zJloxPrWCBX3csB8PQNUxjbR2yWp09ifzBcp6Hzl3hXqvph2b2D9DhOA0poT9OzsSHhj3qOMZjLOemx6d8sgNsvFCiRyO17W0CCzOSPMPsbez4lFSNNYAO9KY6jBDKU0JnGVU7XAM-ZerAgZYTMkb1Srduj8b2lfm5M0jeKdZCICWrmdl08MqsuPz21FKpHHmcgv7L5WK8AgJB6iN8socwvEqX5Eg01sLFpnD1o9ifIz7fkfVNVvA4-14Z4KIJxsepJ4AfCR1EYMets_N9JbRUomuaBKXi-sskA11MYsRyPBw-Uz9vMgPzmRXsb2JGSWQOs09gFPT2miY9191Dk99SVLXsNXCHCNniHudqRmCsblNijV-Vtr_OxsUSRqGavokDEVlHSvVeGfy0M5izHeYzE_74-UVDXFoI_R87S_V_pbQEmiegDsQe9fwmAlE0PJYx7uRgZMT5zjE3UblZqUkdKigF_r2tP-nXSPgK3B6vAy807pW2aFluq0uc4L8m67zfNElYGwg7cYb1y2ppYkFHFm0Mye_5hIyovXUpNcy6gmKy5sFDAvZ8Mpq99HkvGrwaFJ0ADKDFhBigTEaKbtfmL23YhiriicI1vVXTR9nYJsCa9Lu0kw3edmdWx7kqn_zZJ5YvlB6RvO6gk98GppkaHQVZSBOvOkKPmFHSjpo8Zy9lYhkr34_iI7Z7qOnXKx4E91oTfE9Zgds7gptUaanbTGuhULTWuc2dmh2r2O2XIHPD8ZLGjb-ZSwQJKZAZy-dji96NhyMNINDRRxSnK6zudZuKxW7ruVdV6rqQBBOu4hNCMz7oMV4AqRDBhd5ca41tO79HV055A9ngabWGAeGoJ2BJvfUe59TEr_BNRiyemT-9f79Yn5E7mDRcGCEjQkX4KZBBPYMLy550sccn8-Wi2Kqs8i-KXY9EWOJ5P_f0rZRjGggksLAotFQpPXjiDspdtdfiUSyNXLVn0K-OLlzmvqSWU4YGCWrtue9HhKQTcJGCoN5z-a7qSnFiHZu0bWfwFaNROzkFxan3ICZmqBWUg1xFcL0yW0c7HU-G-r9xhVFvh4_FrKI1XpjdTH_g91odcquG894IfnhG4xeos3u_VMqGvmpTppJ3WsdpPZa3GVpJU19nrV0_r8U5UTz2-TKCTsiL22KoH6pnkxkBMbUuFbz322onuODLWwwSWdxskn45A8g6Jmj2m54zZt3XZ1hnPFubKto0eNcCXY1D4t4yXGQNntxxa64ybSZtzrvlkRQq6LYdrzUcagB0BDRWvAKZjKm843EpdCYGNNsDdCVNkoYdoYawa-uL64moZXaF-iqWtsWrSxmV9eHyqwNmj095C06CDg../download [following]\n",
            "--2019-03-21 06:11:42--  https://public.boxcloud.com/d/1/b1!75HpBOZmq_Lp8HQjyzm9H0Zx1Zw5_wzymgFBDYuhB2konfXiQm-TMyKWk7x-9yWsiYHKA4pCdd9UdSiOzyhWSt7sDl13NjB9_FAyhyPXBYEiWyrA68NQpSGrXHbzOu3dfDazSzJRIQEBJ5JNpi4PUdHAiwo5o1zJloxPrWCBX3csB8PQNUxjbR2yWp09ifzBcp6Hzl3hXqvph2b2D9DhOA0poT9OzsSHhj3qOMZjLOemx6d8sgNsvFCiRyO17W0CCzOSPMPsbez4lFSNNYAO9KY6jBDKU0JnGVU7XAM-ZerAgZYTMkb1Srduj8b2lfm5M0jeKdZCICWrmdl08MqsuPz21FKpHHmcgv7L5WK8AgJB6iN8socwvEqX5Eg01sLFpnD1o9ifIz7fkfVNVvA4-14Z4KIJxsepJ4AfCR1EYMets_N9JbRUomuaBKXi-sskA11MYsRyPBw-Uz9vMgPzmRXsb2JGSWQOs09gFPT2miY9191Dk99SVLXsNXCHCNniHudqRmCsblNijV-Vtr_OxsUSRqGavokDEVlHSvVeGfy0M5izHeYzE_74-UVDXFoI_R87S_V_pbQEmiegDsQe9fwmAlE0PJYx7uRgZMT5zjE3UblZqUkdKigF_r2tP-nXSPgK3B6vAy807pW2aFluq0uc4L8m67zfNElYGwg7cYb1y2ppYkFHFm0Mye_5hIyovXUpNcy6gmKy5sFDAvZ8Mpq99HkvGrwaFJ0ADKDFhBigTEaKbtfmL23YhiriicI1vVXTR9nYJsCa9Lu0kw3edmdWx7kqn_zZJ5YvlB6RvO6gk98GppkaHQVZSBOvOkKPmFHSjpo8Zy9lYhkr34_iI7Z7qOnXKx4E91oTfE9Zgds7gptUaanbTGuhULTWuc2dmh2r2O2XIHPD8ZLGjb-ZSwQJKZAZy-dji96NhyMNINDRRxSnK6zudZuKxW7ruVdV6rqQBBOu4hNCMz7oMV4AqRDBhd5ca41tO79HV055A9ngabWGAeGoJ2BJvfUe59TEr_BNRiyemT-9f79Yn5E7mDRcGCEjQkX4KZBBPYMLy550sccn8-Wi2Kqs8i-KXY9EWOJ5P_f0rZRjGggksLAotFQpPXjiDspdtdfiUSyNXLVn0K-OLlzmvqSWU4YGCWrtue9HhKQTcJGCoN5z-a7qSnFiHZu0bWfwFaNROzkFxan3ICZmqBWUg1xFcL0yW0c7HU-G-r9xhVFvh4_FrKI1XpjdTH_g91odcquG894IfnhG4xeos3u_VMqGvmpTppJ3WsdpPZa3GVpJU19nrV0_r8U5UTz2-TKCTsiL22KoH6pnkxkBMbUuFbz322onuODLWwwSWdxskn45A8g6Jmj2m54zZt3XZ1hnPFubKto0eNcCXY1D4t4yXGQNntxxa64ybSZtzrvlkRQq6LYdrzUcagB0BDRWvAKZjKm843EpdCYGNNsDdCVNkoYdoYawa-uL64moZXaF-iqWtsWrSxmV9eHyqwNmj095C06CDg../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.24.200, 107.152.25.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.24.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip   100%[===================>]   3.81G  22.8MB/s    in 2m 51s  \n",
            "\n",
            "2019-03-21 06:14:33 (22.8 MB/s) - ‘hpatches_data.zip’ saved [4088106554/4088106554]\n",
            "\n",
            "Cloning into 'hpatches-benchmark'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 1435 (delta 11), reused 14 (delta 5), pack-reused 1409\u001b[K\n",
            "Receiving objects: 100% (1435/1435), 239.72 MiB | 21.60 MiB/s, done.\n",
            "Resolving deltas: 100% (789/789), done.\n",
            "Checking out files: 100% (135/135), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OfKKVCwawyWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Network\n",
        "\n",
        "The following code block details the finalised network which is used. There is a fixed seed which is used for consistency while evaluating, however can easily be removed. \n",
        "\n",
        "The network is as outlined in the report, with the denoising, feature extraction and descriptors combined into one model."
      ]
    },
    {
      "metadata": {
        "id": "0YF2io-cnthT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Input, UpSampling2D, concatenate, Subtract\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from read_data import HPatches, HPatchesRegularised, DataGeneratorDescRegularised, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, STNHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "import matplotlib.pyplot as plt\n",
        "from layers import BilinearInterpolation\n",
        "from keras.layers import Layer, Lambda\n",
        "\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "# sobel filter\n",
        "def run_sobel(image):\n",
        "  return tf.image.sobel_edges(image)[:,:,:,0]\n",
        "      \n",
        "# initial weights for the STN network\n",
        "def get_initial_weights(output_size):\n",
        "    b = np.zeros((2, 3), dtype='float32')\n",
        "    b[0, 0] = 1\n",
        "    b[1, 1] = 1\n",
        "    W = np.zeros((output_size, 6), dtype='float32')\n",
        "    weights = [W, b.flatten()]\n",
        "    return weights\n",
        "  \n",
        "# Description of the complete model\n",
        "def get_full_model(shape,stn_init=None):  \n",
        "\n",
        "    init_weights = keras.initializers.he_normal()\n",
        "    \n",
        "    # input \n",
        "    inputs = Input(shape)\n",
        "    \n",
        "    # denoise network\n",
        "    depth1  = 32\n",
        "    conv1_1 = Conv2D(depth1, 1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "\n",
        "    # convolution layers\n",
        "    conv1_2  = Conv2D(depth1, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_3  = Conv2D(depth1, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_5  = Conv2D(depth1, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_7  = Conv2D(depth1, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_9  = Conv2D(depth1, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    conv1_11 = Conv2D(depth1, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "\n",
        "    # network\n",
        "    net1 = Subtract()([conv1_1, conv1_2])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_3])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_5])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_7])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_9])\n",
        "    net1 = BatchNormalization()(net1)\n",
        "    net1 = Subtract()([conv1_1, conv1_11])\n",
        "    net1 = BatchNormalization()(net1)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth2 = 16\n",
        "    conv2_1  = Conv2D(depth2, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv2_2  = Conv2D(depth2, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_3  = Conv2D(depth2, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_5  = Conv2D(depth2, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_7  = Conv2D(depth2, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_9  = Conv2D(depth2, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    conv2_11 = Conv2D(depth2, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "\n",
        "    # network\n",
        "    net2 = Subtract()([conv2_1, conv2_2])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_3])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_5])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_7])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_9])\n",
        "    net2 = BatchNormalization()(net2)\n",
        "    net2 = Subtract()([conv2_1, conv2_11])\n",
        "    net2 = BatchNormalization()(net2)  \n",
        "\n",
        "    # convolution layers\n",
        "    depth3 = 8\n",
        "    conv3_1  = Conv2D(depth3, 1, padding = 'same', kernel_initializer = 'he_normal')(net1)\n",
        "    conv3_2  = Conv2D(depth3, 2 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_3  = Conv2D(depth3, 3 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_5  = Conv2D(depth3, 5 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_7  = Conv2D(depth3, 7 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_9  = Conv2D(depth3, 9 , activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    conv3_11 = Conv2D(depth3, 11, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "\n",
        "    # network\n",
        "    net3 = Subtract()([conv3_1, conv3_2])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_3])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_5])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_7])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_9])\n",
        "    net3 = BatchNormalization()(net3)\n",
        "    net3 = Subtract()([conv3_1, conv3_11])\n",
        "    net3 = BatchNormalization()(net3)  \n",
        "\n",
        "  \n",
        "    # stn network    \n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(net3)\n",
        "    locnet = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
        "    locnet = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=init_weights)(locnet)\n",
        "    locnet = Flatten()(locnet)\n",
        "    locnet = Dense(100)(locnet)\n",
        "    locnet = Activation('sigmoid')(locnet)\n",
        "    weights = get_initial_weights(100)\n",
        "    locnet = Dense(6, weights=weights)(locnet)\n",
        "    stn    = BilinearInterpolation(shape[:-1])([net3, locnet])\n",
        "      \n",
        "    # sobel\n",
        "    sobel = Lambda(run_sobel)(net3)\n",
        "    \n",
        "    # features in\n",
        "    l2net = concatenate([ net3, stn , sobel ], axis = -1)\n",
        "    \n",
        "    # L2 Net    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(64, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, strides=2, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1, epsilon=0.0001, scale=False, center=False)(l2net)\n",
        "    l2net = Activation('relu')(l2net)\n",
        "    \n",
        "    l2net = Conv2D(128, 8, padding='valid', input_shape=shape, use_bias = True, kernel_initializer=init_weights)(l2net)\n",
        "    l2net = BatchNormalization(axis = -1)(l2net)\n",
        "\n",
        "    l2net = Reshape((128,))(l2net)\n",
        "  \n",
        "    l2net = Model(inputs = inputs, outputs = l2net)\n",
        "    \n",
        "    descriptor_model = Sequential()\n",
        "    descriptor_model.add(l2net)\n",
        "    \n",
        "    return descriptor_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1QstTDaxTEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Loss function\n",
        "\n",
        "The code block below outlines the loss function used for training. This is similar to the triplet loss function seen in the baseline, however the alpha variable is passed as a parameter to the loss function.\n",
        "\n",
        "As can be seen, an NAdam optimiser os used for the final training.\n"
      ]
    },
    {
      "metadata": {
        "id": "N6a8ibZjnR8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a30bcbf1-ff8a-424d-c5d8-624f7f1033b5"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_full_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "alphaIn = Input(shape=(1,), name='alpha')\n",
        "\n",
        "def triplet_loss_regularised(x):  \n",
        "  a, p, n, _alpha = x\n",
        "\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "\n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha[0]), axis = 1)\n",
        "\n",
        "\n",
        "loss = Lambda(triplet_loss_regularised)([ea, ep, en, alphaIn])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn, alphaIn], outputs=loss)\n",
        "opt = keras.optimizers.SGD(lr=0.1)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=opt)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RxiKk5K4xzvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training and Validation Sets\n",
        "\n",
        "The same `a` split is used for training as with the baseline, so training and test images are the same. Instead of the HPatches data generator, HPatchesRegularised uses the more regularised loss function documented in the report. It contains an anchor, positive and negative image, as well as alpha value based on the images. A regularisation value of 0.75 is chosen for negative pairs of the same image. Batch size is of 50."
      ]
    },
    {
      "metadata": {
        "id": "yjpyrKsnnbje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bb783e36-24d0-4e61-fd18-3266e98f180e"
      },
      "cell_type": "code",
      "source": [
        "!cp ../splits.json /content\n",
        "\n",
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]\n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs))\n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatchesRegularised(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000, batch_size=50)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDescRegularised(0.75,*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000, batch_size=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using noisy patches\n",
            "100%|██████████| 116/116 [00:30<00:00,  3.78it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4392/100000 [00:00<00:02, 43917.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:01<00:00, 64616.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using noisy patches\n",
            "100%|██████████| 116/116 [00:18<00:00,  6.37it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 18776.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yl5gF0YmznM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "The network is trained over 100 epochs, and the models with the best validation score are kept every epoch."
      ]
    },
    {
      "metadata": {
        "id": "pJvLxFWrohDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1169
        },
        "outputId": "78188bd0-b1d8-44b7-9af4-6d709a80a8b9"
      },
      "cell_type": "code",
      "source": [
        "# callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint('data/descriptor_model.weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=100, callbacks=callbacks,\n",
        "                                              verbose=1, validation_data=val_generator)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1536/2000 [======================>.......] - ETA: 1:10 - loss: 0.2108"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7a9b9365856a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=100, callbacks=callbacks,\n\u001b[0;32m----> 6\u001b[0;31m                                               verbose=1, validation_data=val_generator)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TdIGhkvi1o66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Plot\n",
        "\n",
        "Quick plot of training."
      ]
    },
    {
      "metadata": {
        "id": "NHaLjwg2TbWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(descriptor_history.history['loss'])\n",
        "plt.plot(descriptor_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "np.save('data/net_loss.npy', np.array(descriptor_history.history['loss']))\n",
        "np.save('data/net_val_loss.npy', np.array(descriptor_history.history['val_loss']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eN47Yzva1xt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Benchmark\n",
        "\n",
        "The final network is benchmarked using the HPatches benchmark."
      ]
    },
    {
      "metadata": {
        "id": "iYzV5z2BBkN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from get_data import get_data\n",
        "%cd hpatches-benchmark\n",
        "!git pull \n",
        "%cd ..\n",
        "!mkdir -p results\n",
        "\n",
        "generate_desc_csv(descriptor_model, seqs_test, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/deepLearning/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}