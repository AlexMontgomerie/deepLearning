{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/keras_triplet_descriptor/blob/master/Baseline_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "#!rm -rf deepLearning\n",
    "!git clone https://github.com/AlexMontgomerie/deepLearning\n",
    "%cd deepLearning\n",
    "!git pull origin master\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "int(devices)\n",
    "\n",
    "# Taken from\n",
    "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# Colab only provides one GPU and it is not always guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6QbkHnbuIUD"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Input, UpSampling2D, concatenate\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
    "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_descriptor_model(shape):\n",
    "  \n",
    "  '''Architecture copies HardNet architecture'''\n",
    "  \n",
    "  init_weights = keras.initializers.he_normal()\n",
    "  \n",
    "  descriptor_model = Sequential()\n",
    "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "\n",
    "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "\n",
    "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "\n",
    "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "\n",
    "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "\n",
    "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "  descriptor_model.add(BatchNormalization(axis = -1))\n",
    "  descriptor_model.add(Activation('relu'))\n",
    "  descriptor_model.add(Dropout(0.3))\n",
    "\n",
    "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
    "  \n",
    "  # Final descriptor reshape\n",
    "  descriptor_model.add(Reshape((128,)))\n",
    "  \n",
    "  return descriptor_model\n",
    "  \n",
    "\n",
    "from keras.layers import Lambda\n",
    "shape = (32, 32, 1)\n",
    "xa = Input(shape=shape, name='a')\n",
    "xp = Input(shape=shape, name='p')\n",
    "xn = Input(shape=shape, name='n')\n",
    "descriptor_model = get_descriptor_model(shape)\n",
    "ea = descriptor_model(xa)\n",
    "ep = descriptor_model(xp)\n",
    "en = descriptor_model(xn)\n",
    "\n",
    "  \n",
    "def triplet_loss(x):\n",
    "  \n",
    "  output_dim = 128\n",
    "  a, p, n = x\n",
    "  _alpha = 1.0\n",
    "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
    "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
    "  \n",
    "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)\n",
    "\n",
    "\n",
    "loss = Lambda(triplet_loss)([ea, ep, en])\n",
    "\n",
    "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "sgd = keras.optimizers.SGD(lr=0.1)\n",
    "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpatches_dir = './hpatches'\n",
    "splits_path = './splits.json'\n",
    "\n",
    "splits_json = json.load(open(splits_path, 'rb'))\n",
    "split = splits_json['a']\n",
    "\n",
    "train_fnames = split['train']\n",
    "test_fnames = split['test']\n",
    "\n",
    "seqs = glob.glob(hpatches_dir+'/*')\n",
    "seqs = [os.path.abspath(p) for p in seqs]   \n",
    "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
    "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))\n",
    "\n",
    "\n",
    "### Descriptor loading and training\n",
    "# Loading images\n",
    "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
    "                    denoise_model=denoise_model, use_clean=False)\n",
    "# Creating training generator\n",
    "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
    "# Creating validation generator\n",
    "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)\n",
    "epochs = 50\n",
    "### As with the denoising model, we use a loop to save for each epoch \n",
    "## #the weights in an external website in case colab stops. \n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "\n",
    "### If you have a model saved from a previous training session\n",
    "### Load it in the next line\n",
    "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
    "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "  descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=1, verbose=1, validation_data=val_generator)\n",
    "  \n",
    "  ### Saves optimizer and weights\n",
    "  descriptor_model_trip.save('descriptor.h5') \n",
    "  from google.colab import files\n",
    "  files.download('denoise_add.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Baseline_code.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
